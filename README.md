
| NAME | PROMPT | DESCRIPTION | EXAMPLE |
|----|----|----|----|
| app.yaml | Generate a Kubernetes **Pod** named `app` in the default namespace. Run one container `app` on image `gcr.io/k8s-k3s/demo:v1.0.0`, expose **containerPort 8000** (`name: http`), add labels `app: demo` & `run: demo`. Return only YAML. | Minimal single-Pod workload for quick tests or CI. No auto-heal (unlike a Deployment) — illustrates the absolute basics of a K8s manifest. | [yaml/app.yaml](yaml/app.yaml) |
| app-livenessProbe.yaml | Generate a **Pod** named `app-livenessprob` in namespace `demo` that runs one container **app** on image `gcr.io/smartcity-gl/demo:v1.0.1`. Expose container port **8080** (`name: http`). Configure an HTTP **livenessProbe** hitting `/` on port **8000** with `initialDelaySeconds: 5`, `timeoutSeconds: 1`, `periodSeconds: 10`, `failureThreshold: 3`. Return only YAML. | Shows a full liveness probe: kubelet restarts the container after 3 failed checks, protecting against “hung” processes. Port 8080 serves the app, 8000 is dedicated for health. | [yaml/app-livenessProbe.yaml](yaml/app-livenessProbe.yaml) |
| app-readinessProbe.yaml | Generate a Kubernetes Pod manifest that meets \*\*all\*\* of the following requirements ­— do not omit any section:1. \`apiVersion: v1\`, \`kind: Pod\`, \`metadata.name: app-readinessprob\` (default namespace).2. A single container named \*\*app\*\* using image \`gcr.io/smartcity-gl/harmonic:v2.0.2\`.3. \*\*Add this ports section exactly:\*\*   ports:     - containerPort: 8000       name: http4. Configure an HTTP \*\*livenessProbe\*\* hitting \`/\` on port \*\*8000\*\* with     \`initialDelaySeconds: 5\`, \`timeoutSeconds: 1\`, \`periodSeconds: 10\`, \`failureThreshold: 3\`.5. Configure an HTTP \*\*readinessProbe\*\* hitting \`/ready\` on port \*\*8000\*\* with     \`initialDelaySeconds: 0\`, \`periodSeconds: 2\`, \`successThreshold: 1\`, \`failureThreshold: 3\`.6. Return \*\*only YAML\*\*, enclosed in triple back-ticks, no additional commentary. | Combines liveness (self-heal on hang) and readiness (traffic held until `/ready` responds), avoiding 5xx during cold-starts and ensuring auto-restart of frozen containers. | [yaml/app-readinessProbe.yaml](yaml/app-readinessProbe.yaml) |
| app-volumeMounts.yaml | Generate a Kubernetes Pod manifest that satisfies \*\*every item below exactly — no extra resources\*\*.1. apiVersion: v1, kind: Pod, metadata.name: \`app-volume\` (default namespace).2. One container called \*\*app\*\* running image \`gcr.io/kuar-demo/kuard-amd64:1\`.3. Expose \*\*containerPort 8080\*\* with \`name: http\`.4. Add \*\*livenessProbe\*\*:   - httpGet path \`/healthy\`, port 8080   - initialDelaySeconds 5, timeoutSeconds 1, periodSeconds 10, failureThreshold 35. Add \*\*readinessProbe\*\*:   - httpGet path \`/ready\`, port 8080   - initialDelaySeconds 0, periodSeconds 2, successThreshold 1, failureThreshold 36. Mount a volume:   - volumeMounts:       - mountPath: "/data"         name: data   - volumes:       - name: data         hostPath:           path: "/var/lib/app"7. Do \*\*NOT\*\* create PersistentVolumeClaim, StorageClass, or any extra objects.8. Preserve key order as shown and return \*\*only YAML inside triple back-ticks\*\*, nothing else. | Demonstrates durable storage via PVC + `volumeMounts`; preferred over `hostPath` for production. | [yaml/app-volumeMounts.yaml](yaml/app-volumeMounts.yaml) |
| app-cronjob.yaml | Generate a **CronJob** named `app-cronjob` with `apiVersion: batch/v1beta1`. It must run every **5 minutes** (`schedule: "*/5 * * * *"`). The job template should create one container `hello` using the `bash` image and execute the command `["echo", "Hello world"]`. Set `restartPolicy: OnFailure`. Return only YAML. | Minimal demo CronJob that prints “Hello world” every five minutes; useful as a heartbeat or template starter. Uses the historical `v1beta1` API to match legacy clusters <1.25. | [yaml/app-cronjob.yaml](yaml/app-cronjob.yaml) |
| app-job.yaml | Generate a **Job** named `app-job` (`apiVersion: batch/v1`) with `backoffLimit: 0`. The Pod template must: ① declare two volumes — `data-input` from a GCE persistent disk (`pdName: glow-data-disk-200`, `fsType: ext4`) and `data-output` as `emptyDir`; ② run an **initContainer** `init` on image `google/cloud-sdk:275.0.0-alpine` that executes `gsutil -m rsync -dr gs://glow-sportradar/ /data/input`, mounting `data-input` at `/data/input`; ③ run a main container **processor** on image `glowtools/soccer:latest-4e4254a` with command `["/bin/sh","-c","ls -l /data/input"]`, mounting both volumes (`/data/input`, `/data/output`); ④ add lifecycle hooks: `postStart` writes “Start from the postStart handler” to `/data/input/message`, `preStop` writes “Stop from the postStop handler” to `/data/output/message`; ⑤ set `restartPolicy: Never`. Return only YAML. | Full-feature batch Job: pulls data from GCS via init container, processes it, logs via lifecycle hooks, illustrates multi-volume mounts (GCE PD + emptyDir) and zero-retry strategy for fast failure feedback. | [yaml/app-job.yaml](yaml/app-job.yaml) |
| app-multicontainer.yaml | Generate a **Pod** named `app-two-containers` with one `emptyDir` volume `html`. It must run two containers: **1st** on image `nginx` mounting `html` at `/usr/share/nginx/html`, and **2nd** on image `debian` mounting the same volume at `/html` and executing an endless shell loop that appends the current date to `/html/index.html` every second (`command: ["/bin/sh","-c"]`, `args` as the while-loop). No additional ports or resources. Return only YAML. | Demonstrates the “sidecar writer” pattern: helper container continuously generates content, while Nginx serves it from a shared in-memory volume — handy for live-log viewers or real-time dashboards. | [yaml/app-multicontainer.yaml](yaml/app-multicontainer.yaml) |
| app-resources.yaml | Generate a **Pod** named `app-resource` that runs one container **app** on image `gcr.io/kuar-demo/kuard-amd64:1`. Expose container port **8080** (`name: http`). Configure an HTTP **livenessProbe** on `/healthy` (port 8080) with `initialDelaySeconds: 5`, `timeoutSeconds: 1`, `periodSeconds: 10`, `failureThreshold: 3`, and an HTTP **readinessProbe** on `/ready` (port 8080) with `initialDelaySeconds: 0`, `periodSeconds: 2`, `successThreshold: 1`, `failureThreshold: 3`. Set **resource requests** `cpu: 100m`, `memory: 128Mi` and **limits** `cpu: 1000m`, `memory: 256Mi`. Return only YAML. | Shows practical right-sizing plus full health-checks: scheduler guarantees 100 m/128 Mi, but container can burst to one full core; probes keep it auto-healing and traffic-gated. | [yaml/app-resources.yaml](yaml/app-resources.yaml) |
| app-secret-env.yaml | Generate a **Pod** named `app-secret-env` that runs one container **mycontainer** on image `redis`. Inject two environment variables using an **existing Secret** `mysecret1`: `SECRET_USERNAME` ← key `username`, `SECRET_PASSWORD` ← key `password`, via individual `env` entries with `valueFrom.secretKeyRef`. Set `restartPolicy: Never`. Return only YAML. | Shows how to pull specific keys from a pre-created Secret into environment variables — credentials stay in `mysecret1`, not in the image or manifest. | [yaml/app-secret-env.yaml](yaml/app-secret-env.yaml) |
| app-configmap.yaml | Generate a **Pod** named `app-config` that runs one container **mypod** on image `redis` (`imagePullPolicy: Always`). Inside `spec`, maintain **this exact key order**: \n ①`containers:`block, \n ②`volumes:`block, \n ③`restartPolicy:`. \n Container requirements: • mount a volume`config-volume` (sourced from an existing ConfigMap `app-config`) at `/config`; • expose env var `CONFIGMAP_PARAM` using `env.valueFrom.configMapKeyRef` (`name: app-config`, `key: config-param`). Declare the volume in `spec.volumes`; set `restartPolicy: Never`. Return only YAML. | Shows how to consume a pre-created ConfigMap both as files (volume mount) and as a single key-to-env-var injection, while demonstrating strict field ordering in the manifest. | [yaml/app-configmap.yaml](yaml/app-configmap.yaml) |
| app-job-processor.yaml | Generate a Job `app-job-processor` (apiVersion batch/v1, backoffLimit 0) whose Pod template declares volumes: data-input→PVC **sportradar-pvc-input**, data-output→PVC **sportradar-pvc-output**, Secrets **data-params**, **metrics-params**, **storage-secret** (as data-secret), **soccer-code** (as data-code), **metrics-code**, **gcp-sa** (as gcp-auth), **gsutil-conf**. Add initContainer **init** (image `google/cloud-sdk:275.0.0-alpine`) command `"cp /tmp/.boto /root/ && gsutil -m rsync -dr gs://glow-sportradar/ /tmp/data/input"`, mounting: `data-input`→`/tmp/data/input`, `gcp-auth`→`/tmp/data/auth`, `gsutil-conf`→`/tmp`. Then run containers: **main** (`busybox`, `echo Finish`); **processor** (`glowtools/soccer:latest-122f4de`) command `"PYTHONPATH=/home/app/python:/home/app/function python /tmp/data/code/handler.py $DEBUG -conf /tmp/data/conf/conf"`, env `SECRET_NAME=/tmp/data/secrets/secret`, `FAAS_SECRET_PATH=/tmp/data/secrets`, `DEBUG=`; mounts `data-code`→`/tmp/data/code` (RO), `data-params`→`/tmp/data/conf` (RO), `data-secret`→`/tmp/data/secrets` (RO), `data-input`→`/tmp/data/input` (RO), `data-output`→`/tmp/data/output`; **metrics** (`glowtools/soccermetrics:latest-990f291`) command `"PYTHONPATH=/home/app/python:/home/app/function python /tmp/data/code/handler.py -conf /tmp/data/conf/conf"`, mounts `metrics-code`→`/tmp/data/code` (RO), `metrics-params`→`/tmp/data/conf` (RO), `data-input`→`/tmp/data/input` (RO), `data-output`→`/tmp/data/output`. Set `restartPolicy: Never`. Return **only YAML**. | Full-feature batch pipeline: init container syncs data from GCS; processor & metrics containers run Python handlers over shared datasets; busybox signals completion — showcases multi-volume, multi-secret orchestration in a single Job. | [yaml/app-job-processor.yaml](yaml/app-job-processor.yaml) |
| app-job-rsync.yaml | Generate a **Job** named `app-job-rsync` (apiVersion batch/v1, backoffLimit 0). The Pod template must: ① declare a volume **data-input** sourced from a GCE PD (`pdName: glow-data-disk-200`, `fsType: ext4`); ② run a single container **init** on image `google/cloud-sdk:275.0.0-alpine` with command `"/bin/sh -c 'gsutil -m rsync -dr gs://glow-sportradar/ /data/input'"`; ③ mount the volume at `/data/input`; ④ set `restartPolicy: Never`. Return **only YAML**. | One-shot rsync job that pulls data from GCS into a node-attached persistent disk — perfect for seeding datasets before downstream processing. | [yaml/app-job-rsync.yaml](yaml/app-job-rsync.yaml) |
| app-pv.yaml | Generate **four resources**: ① `PersistentVolume` **sportradar-pv-input** (`capacity: 10Gi`, `volumeMode: Filesystem`, `accessModes: [ReadWriteOnce]`, `storageClassName: local-storage`, `persistentVolumeReclaimPolicy: Delete`, `local.path: /data/sportradar/data/input`, `nodeAffinity` hostname =`glow-vgpu-vm`); ② `PersistentVolume` **sportradar-pv-output** (same spec but `local.path: /data/sportradar/data/output`); ③ `PersistentVolumeClaim` **sportradar-pvc-input** requesting **10Gi** RWO, Filesystem, `storageClassName: local-storage`; ④ `PersistentVolumeClaim` **sportradar-pvc-output** requesting **2Gi** RWO, Filesystem, `storageClassName: local-storage`. Return **only YAML**. | Shows static local-storage provisioning with node affinity: two pre-bound PVs on a specific bare-metal node and matching PVCs for input/output data paths. | [yaml/app-pv.yaml](yaml/app-pv.yaml) |
| app-secret.yaml | Generate a **Pod** named `app-secret` that runs one container **mypod** on image `redis`. The container must mount a volume **foo** sourced from an existing Secret `secrret1` at mountPath `/etc/foo` with `readOnly: true`. Declare the volume in `spec.volumes`; no other resources. Return **only YAML**. | Demonstrates the simplest Secret-as-volume pattern: sensitive files (certs, keys, configs) are surfaced read-only inside the container without baking them into the image. | [yaml/app-secret.yaml](yaml/app-secret.yaml) |


